{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "miniproj3_PGGAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmk2aG_jqUUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import sqrt\n",
        "from numpy import load\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from skimage.transform import resize\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import UpSampling2D\n",
        "from keras.layers import AveragePooling2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Layer\n",
        "from keras.layers import Add\n",
        "from keras.constraints import max_norm\n",
        "from keras.initializers import RandomNormal\n",
        "from keras import backend\n",
        "from matplotlib import pyplot\n",
        "from numpy import array\n",
        "from numpy.random import uniform\n",
        "from PIL import Image\n",
        "from keras import losses\n",
        "from keras import activations\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk7H0D4Er2-S",
        "colab_type": "text"
      },
      "source": [
        "Mounting Google colab and getting trainingdatasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2ZdIyPd0jYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "52af9571-922f-4129-84cc-fcea458e5e8d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/alireza.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/alireza\")\n",
        "zip_ref.close()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNoyS0eFr2Hm",
        "colab_type": "text"
      },
      "source": [
        "getting 50000 face image from dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3kL0RNf28S3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "faces = []\n",
        "for i in range(50000):\n",
        "  # load the image\n",
        "  image = Image.open(\"/content/alireza/alireza/\" + str(i) + \".jpg\")\n",
        "  image = image.convert('RGB')\n",
        "  faces.append(asarray(image))\n",
        "faces = array(faces)\n",
        "print('Loaded: ', faces.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAevJu_LsP-w",
        "colab_type": "text"
      },
      "source": [
        "show 100 face images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs__ph316J4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_faces(faces, n):\n",
        "  pyplot.figure(figsize=(20, 20))\n",
        "  for i in range(n * n):\n",
        "    pyplot.subplot(n, n, 1 + i)\n",
        "    # turn off axis\n",
        "    pyplot.axis('off')\n",
        "    # plot raw pixel data\n",
        "    pyplot.imshow(faces[i].astype('uint8'))\n",
        "  pyplot.show()\n",
        "plot_faces(faces, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCKSypn7sVvZ",
        "colab_type": "text"
      },
      "source": [
        "# PixelNormalization\n",
        "defining a Pixel Normalization layer which calculates normalization factor  by mean square values of each pixel throgh all feature maps and devide all inputs  by normalization factor "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu7XaBKHtl_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pixel-wise feature vector normalization layer\n",
        "class PixelNormalization(Layer):\n",
        "\t# initialize the layer\n",
        "\tdef __init__(self, **kwargs):\n",
        "\t\tsuper(PixelNormalization, self).__init__(**kwargs)\n",
        " \n",
        "\t# perform the operation\n",
        "\tdef call(self, inputs):\n",
        "\t\t# calculate square pixel values\n",
        "\t\tvalues = inputs**2.0\n",
        "\t\t# calculate the mean pixel values\n",
        "\t\tmean_values = backend.mean(values, axis=-1, keepdims=True)\n",
        "\t\t# ensure the mean is not zero\n",
        "\t\tmean_values += 1.0e-8\n",
        "\t\t# calculate the sqrt of the mean squared value (L2 norm)\n",
        "\t\tl2 = backend.sqrt(mean_values)\n",
        "\t\t# normalize values by the l2 norm\n",
        "\t\tnormalized = inputs / l2\n",
        "\t\treturn normalized\n",
        " \n",
        "\t# define the output shape of the layer\n",
        "\tdef compute_output_shape(self, input_shape):\n",
        "\t\treturn input_shape\n",
        " "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XrVGW_1toCs",
        "colab_type": "text"
      },
      "source": [
        "# MinibatchStdev\n",
        "in order to increase generator variations define minibatchStdev layer which calculates standard deviation of each feature map through all minibatch then calculates the average of this standard deviation for all feature maps and concatenate the final average to feature maps as a new feature map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug64KcU-uWo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# mini-batch standard deviation layer\n",
        "class MinibatchStdev(Layer):\n",
        "\t# initialize the layer\n",
        "\tdef __init__(self, **kwargs):\n",
        "\t\tsuper(MinibatchStdev, self).__init__(**kwargs)\n",
        " \n",
        "\t# perform the operation\n",
        "\tdef call(self, inputs):\n",
        "\t\t# calculate the mean value for each pixel across channels\n",
        "\t\tmean = backend.mean(inputs, axis=0, keepdims=True)\n",
        "\t\t# calculate the squared differences between pixel values and mean\n",
        "\t\tsqu_diffs = backend.square(inputs - mean)\n",
        "\t\t# calculate the average of the squared differences (variance)\n",
        "\t\tmean_sq_diff = backend.mean(squ_diffs, axis=0, keepdims=True)\n",
        "\t\t# add a small value to avoid a blow-up when we calculate stdev\n",
        "\t\tmean_sq_diff += 1e-4\n",
        "\t\t# square root of the variance (stdev)\n",
        "\t\tstdev = backend.sqrt(mean_sq_diff)\n",
        "\t\t# calculate the mean standard deviation across each pixel coord\n",
        "\t\tmean_pix = backend.mean(stdev, keepdims=True)\n",
        "\t\t# scale this up to be the size of one input feature map for each sample\n",
        "\t\tshape = backend.shape(inputs)\n",
        "\t\toutput = backend.tile(mean_pix, (shape[0], shape[1], shape[2], 1))\n",
        "\t\t# concatenate with the output\n",
        "\t\tcombined = backend.concatenate([inputs, output], axis=-1)\n",
        "\t\treturn combined\n",
        " \n",
        "\t# define the output shape of the layer\n",
        "\tdef compute_output_shape(self, input_shape):\n",
        "\t\t# create a copy of the input shape as a list\n",
        "\t\tinput_shape = list(input_shape)\n",
        "\t\t# add one to the channel dimension (assume channels-last)\n",
        "\t\tinput_shape[-1] += 1\n",
        "\t\t# convert list to a tuple\n",
        "\t\treturn tuple(input_shape)\n",
        " "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y9rghtWvLts",
        "colab_type": "text"
      },
      "source": [
        "# WeightedSum\n",
        "using weightedSum layer to calculate the weighted average of old blocks and new added blocks during fade-in phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7Cytn18vt9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# weighted sum output\n",
        "class WeightedSum(Add):\n",
        "\t# init with default value\n",
        "\tdef __init__(self, alpha=0.0, **kwargs):\n",
        "\t\tsuper(WeightedSum, self).__init__(**kwargs)\n",
        "\t\tself.alpha = backend.variable(alpha, name='ws_alpha')\n",
        " \n",
        "\t# output a weighted sum of inputs\n",
        "\tdef _merge_function(self, inputs):\n",
        "\t\t# only supports a weighted sum of two inputs\n",
        "\t\tassert (len(inputs) == 2)\n",
        "\t\t# ((1-a) * input1) + (a * input2)\n",
        "\t\toutput = ((1.0 - self.alpha) * inputs[0]) + (self.alpha * inputs[1])\n",
        "\t\treturn output\n",
        " "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA9at5YywDaY",
        "colab_type": "text"
      },
      "source": [
        "#wasserstein_loss\n",
        "calculating wasserstein loss\n",
        "fake image labels are -1 and real image labels are +1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4ZkVpfqwIt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate wasserstein loss\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "\treturn backend.mean(y_pred * y_true)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz6GnWHKwWho",
        "colab_type": "text"
      },
      "source": [
        "# defining Discriminator\n",
        "first we define a base discriminator block containing 128 1x1,3x3 and 4x4 Conv2d filters and one averagePooling layer. each have leakyRelu activation function and we define one MinibatchStdev in the last layer.\n",
        "\n",
        "\n",
        "in each scale up phase we add a discriminator block with 128 1x1,3x3 and 3x3 conv2D filters and one averge pooling layer each have leakyRelu activation function.\n",
        "\n",
        "\n",
        "we define one WeightedSum layer to compute the weighted average of new and old block.\n",
        "\n",
        "finally we define a list of pair of models which contains one model to be trained  in simple phase and one to be trained in feade-in to new model phase\n",
        "\n",
        "Notes:\n",
        "\n",
        "we use an MaxNorm constraint on weights which clips weight in [-1,1] range instead of using equalized learning rate which do the same scaling\n",
        "\n",
        "and we initialize weights from N(0,0.02) ditribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjASLaHN4Mkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# add a discriminator block\n",
        "def add_discriminator_block(old_model, n_input_layers=3):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# weight constraint\n",
        "\tconst = max_norm(1.0)\n",
        "\t# get shape of existing model\n",
        "\tin_shape = list(old_model.input.shape)\n",
        "\t# define new input shape as double the size\n",
        "\tinput_shape = (in_shape[-2]*2, in_shape[-2]*2, in_shape[-1])\n",
        "\tin_image = Input(shape=input_shape)\n",
        "\t# define new input processing layer\n",
        "\td = Conv2D(128, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(in_image)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# define new block\n",
        "\td = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\td = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\td = AveragePooling2D()(d)\n",
        "\tblock_new = d\n",
        "\t# skip the input, 1x1 and activation for the old model\n",
        "\tfor i in range(n_input_layers, len(old_model.layers)):\n",
        "\t\td = old_model.layers[i](d)\n",
        "\t# define straight-through model\n",
        "\tmodel1 = Model(in_image, d)\n",
        "\t# compile model\n",
        "\tmodel1.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8), metrics=['accuracy'])\n",
        "\t# downsample the new larger image\n",
        "\tdownsample = AveragePooling2D()(in_image)\n",
        "\t# connect old input processing to downsampled new input\n",
        "\tblock_old = old_model.layers[1](downsample)\n",
        "\tblock_old = old_model.layers[2](block_old)\n",
        "\t# fade in output of old model input layer with new input\n",
        "\td = WeightedSum()([block_old, block_new])\n",
        "\t# skip the input, 1x1 and activation for the old model\n",
        "\tfor i in range(n_input_layers, len(old_model.layers)):\n",
        "\t\td = old_model.layers[i](d)\n",
        "\t# define straight-through model\n",
        "\tmodel2 = Model(in_image, d)\n",
        "\t# compile model\n",
        "\tmodel2.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8), metrics=['accuracy'])\n",
        "\treturn [model1, model2]\n",
        " \n",
        "# define the discriminator models for each image resolution\n",
        "def define_discriminator(n_blocks, input_shape=(4,4,3)):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# weight constraint\n",
        "\tconst = max_norm(1.0)\n",
        "\tmodel_list = list()\n",
        "\t# base model input\n",
        "\tin_image = Input(shape=input_shape)\n",
        "\t# conv 1x1\n",
        "\td = Conv2D(128, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(in_image)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# conv 3x3 (output block)\n",
        "\td = MinibatchStdev()(d)\n",
        "\td = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# conv 4x4\n",
        "\td = Conv2D(128, (4,4), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# dense output layer\n",
        "\td = Flatten()(d)\n",
        "\tout_class = Dense(1, activation='tanh')(d)\n",
        "\t# define model\n",
        "\tmodel = Model(in_image, out_class)\n",
        "\t# compile model\n",
        "\tmodel.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
        "\t# store model\n",
        "\tmodel_list.append([model, model])\n",
        "\t# create submodels\n",
        "\tfor i in range(1, n_blocks):\n",
        "\t\t# get prior model without the fade-on\n",
        "\t\told_model = model_list[i - 1][0]\n",
        "\t\t# create new model for next resolution\n",
        "\t\tmodels = add_discriminator_block(old_model)\n",
        "\t\t# store model\n",
        "\t\tmodel_list.append(models)\n",
        "\treturn model_list\n",
        " "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go4qYK2R4Mxw",
        "colab_type": "text"
      },
      "source": [
        "# Defining generator block\n",
        "first we define a base generator block which contains a Dense and reshape layer and 128 3x3,3x3 and 1x1 filters each have a leakyRelu activation function and followed by a PixelNormalization layer.\n",
        "\n",
        "in each scaleup step we add a generator block which contains 128 3x3,3x3 and 1x1 conv2D each have a leakyRelu activation function and followed by a PixelNormalization layer\n",
        "\n",
        "for fade-in phase we add a wightedSum layer to compute weighted average of new and old block\n",
        "\n",
        "finally we have a list of pairs of generator models one used in simple learning phase and another one used in fade-in phase.\n",
        "\n",
        "Notes:\n",
        "\n",
        "we use an MaxNorm constraint on weights which clips weight in [-1,1] range instead of using equalized learning rate which do the same scaling\n",
        "\n",
        "and we initialize weights from N(0,0.02) ditribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e8LZQTNwWsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# add a generator block\n",
        "def add_generator_block(old_model):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# weight constraint\n",
        "\tconst = max_norm(1.0)\n",
        "\t# get the end of the last block\n",
        "\tblock_end = old_model.layers[-2].output\n",
        "\t# upsample, and define new block\n",
        "\tupsampling = UpSampling2D()(block_end)\n",
        "\tg = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(upsampling)\n",
        "\tg = PixelNormalization()(g)\n",
        "\tg = LeakyReLU(alpha=0.2)(g)\n",
        "\tg = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
        "\tg = PixelNormalization()(g)\n",
        "\tg = LeakyReLU(alpha=0.2)(g)\n",
        "\t# add new output layer\n",
        "\tout_image = Conv2D(3, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const, activation='tanh')(g)\n",
        "\t# define model\n",
        "\tmodel1 = Model(old_model.input, out_image)\n",
        "\t# get the output layer from old model\n",
        "\tout_old = old_model.layers[-1]\n",
        "\t# connect the upsampling to the old output layer\n",
        "\tout_image2 = out_old(upsampling)\n",
        "\t# define new output image as the weighted sum of the old and new models\n",
        "\tmerged = WeightedSum()([out_image2, out_image])\n",
        "\t# define model\n",
        "\tmodel2 = Model(old_model.input, merged)\n",
        "\treturn [model1, model2]\n",
        " \n",
        "# define generator models\n",
        "def define_generator(latent_dim, n_blocks, in_dim=4):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# weight constraint\n",
        "\tconst = max_norm(1.0)\n",
        "\tmodel_list = list()\n",
        "\t# base model latent input\n",
        "\tin_latent = Input(shape=(latent_dim,))\n",
        "\t# linear scale up to activation maps\n",
        "\tg  = Dense(128 * in_dim * in_dim, kernel_initializer=init, kernel_constraint=const)(in_latent)\n",
        "\tg = Reshape((in_dim, in_dim, 128))(g)\n",
        "\t# conv 4x4, input block\n",
        "\tg = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
        "\tg = PixelNormalization()(g)\n",
        "\tg = LeakyReLU(alpha=0.2)(g)\n",
        "\t# conv 3x3\n",
        "\tg = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
        "\tg = PixelNormalization()(g)\n",
        "\tg = LeakyReLU(alpha=0.2)(g)\n",
        "\t# conv 1x1, output block\n",
        "\tout_image = Conv2D(3, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const, activation='tanh')(g)\n",
        "\t# define model\n",
        "\tmodel = Model(in_latent, out_image)\n",
        "\t# store model\n",
        "\tmodel_list.append([model, model])\n",
        "\t# create submodels\n",
        "\tfor i in range(1, n_blocks):\n",
        "\t\t# get prior model without the fade-on\n",
        "\t\told_model = model_list[i - 1][0]\n",
        "\t\t# create new model for next resolution\n",
        "\t\tmodels = add_generator_block(old_model)\n",
        "\t\t# store model\n",
        "\t\tmodel_list.append(models)\n",
        "\treturn model_list\n",
        " "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo5uw-yA78bI",
        "colab_type": "text"
      },
      "source": [
        "# training generator with discriminator\n",
        "\n",
        "in order to train generator we need a critic to compute the loss for generated images se we create a composite model containing generator and discriminator, but we just update generators weights and remain the discriminators weight unchanged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XskwbUe78iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define composite models for training generators via discriminators\n",
        "def define_composite(discriminators, generators):\n",
        "\tmodel_list = list()\n",
        "\t# create composite models\n",
        "\tfor i in range(len(discriminators)):\n",
        "\t\tg_models, d_models = generators[i], discriminators[i]\n",
        "\t\t# straight-through model\n",
        "\t\td_models[0].trainable = False\n",
        "\t\tmodel1 = Sequential()\n",
        "\t\tmodel1.add(g_models[0])\n",
        "\t\tmodel1.add(d_models[0])\n",
        "\t\tmodel1.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
        "\t\t# fade-in model\n",
        "\t\td_models[1].trainable = False\n",
        "\t\tmodel2 = Sequential()\n",
        "\t\tmodel2.add(g_models[1])\n",
        "\t\tmodel2.add(d_models[1])\n",
        "\t\tmodel2.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
        "\t\t# store\n",
        "\t\tmodel_list.append([model1, model2])\n",
        "\treturn model_list\n",
        " "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2CKAOqa9gjh",
        "colab_type": "text"
      },
      "source": [
        "# Generating real samples with required shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOikc_dA9gri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        " \n",
        "# select real samples\n",
        "def generate_real_samples(data_shape, n_samples):\n",
        "\t# choose random instances\n",
        "\tix = randint(0, 50000, n_samples)\n",
        "\tfaces = []\n",
        "\tfor x in ix:\n",
        "\t\timage = Image.open(\"/content/alireza/alireza/\" + str(x) + \".jpg\")\n",
        "\t\timage = image.convert('RGB')\n",
        "\t\timage = asarray(image)\n",
        "\t\timage = resize(image, data_shape, 0)\n",
        "\t\tfaces.append(image)\n",
        "\tfaces = array(faces)\n",
        "\tfaces = faces.astype('float32')\n",
        "\tfaces = (faces - 127.5) / 127.5\n",
        "\t# select images\n",
        "\t#X = dataset[ix]\n",
        "\t# generate class labels\n",
        "\ty = ones((n_samples, 1))\n",
        "\treturn faces, y\n",
        " "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTTXfkwn9tkV",
        "colab_type": "text"
      },
      "source": [
        "# Generating fake samles\n",
        "\n",
        "first we generate a noise vector and the pass it to generator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww2B5oaU9tv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = uniform(-1,1,latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        " \n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "\t# generate points in latent space\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\tX = generator.predict(x_input)\n",
        "\t# create class labels\n",
        "\ty = -ones((n_samples, 1))\n",
        "\treturn X, y\n",
        " "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RowLhpbC-XbO",
        "colab_type": "text"
      },
      "source": [
        "# setting alpha in fade-in phase\n",
        "\n",
        "during fade-in phase we change alpha linearly from 0 to 1 which means at first we get more weight to old block but littel y little we get more weight to new block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9nyn3f5-Xlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# update the alpha value on each instance of WeightedSum\n",
        "def update_fadein(models, step, n_steps):\n",
        "\t# calculate current alpha (linear from 0 to 1)\n",
        "\talpha = step / float(n_steps - 1)\n",
        "\t# update the alpha for each model\n",
        "\tfor model in models:\n",
        "\t\tfor layer in model.layers:\n",
        "\t\t\tif isinstance(layer, WeightedSum):\n",
        "\t\t\t\tbackend.set_value(layer.alpha, alpha)\n",
        " "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95ljCD5JAWqv",
        "colab_type": "text"
      },
      "source": [
        "# train epochs\n",
        "to train PGGAN model we first train discriminator 2 half-batches one half-batch for real samples with 1 label and another half-batch for fake samples with label -1 and then we train generator fake samples with label 1 which means discriminator must detect them real.\n",
        "\n",
        "during fade-in phase alpha updates linearly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnNEYg4OAKiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# train a generator and discriminator\n",
        "def train_epochs(g_model, d_model, gan_model, data_shape, n_epochs, n_batch, fadein=False):\n",
        "\t# calculate the number of batches per training epoch\n",
        "\tbat_per_epo = int(50000 / n_batch)\n",
        "\t# calculate the number of training iterations\n",
        "\tn_steps = bat_per_epo * n_epochs\n",
        "\t# calculate the size of half a batch of samples\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\t# manually enumerate epochs\n",
        "\tfor j in range(n_epochs):\n",
        "\t\tfor i in range(bat_per_epo):\n",
        "\t\t\t# update alpha for all WeightedSum layers when fading in new blocks\n",
        "\t\t\tif fadein:\n",
        "\t\t\t\tupdate_fadein([g_model, d_model, gan_model], i, n_steps)\n",
        "\t\t\t# prepare real and fake samples\n",
        "\t\t\tX_real, y_real = generate_real_samples(data_shape, half_batch)\n",
        "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t\t# update discriminator model\n",
        "\t\t\td_loss1 = d_model.train_on_batch(X_real, y_real)\n",
        "\t\t\td_loss2 = d_model.train_on_batch(X_fake, y_fake)\n",
        "\t\t\t# update the generator via the discriminator's error\n",
        "\t\t\tz_input = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t\ty_real2 = ones((n_batch, 1))\n",
        "\t\t\tg_loss = gan_model.train_on_batch(z_input, y_real2)\n",
        "\t\t\t# summarize loss on this batch\n",
        "\t\t\tprint('>%d, d1=%.5f, d2=%.5f g=%.5f' % (i+1, d_loss1, d_loss2, g_loss))\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SxzW_rOCA5K",
        "colab_type": "text"
      },
      "source": [
        "# Summarize performance\n",
        "\n",
        "save model and save a plot of generated images for each images size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn25Mj5jCB0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# generate samples and save as a plot and save the model\n",
        "def summarize_performance(status, g_model, latent_dim, n_samples=25):\n",
        "\t# devise name\n",
        "\tgen_shape = g_model.output_shape\n",
        "\tname = '%03dx%03d-%s' % (gen_shape[1], gen_shape[2], status)\n",
        "\t# generate images\n",
        "\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "\t# normalize pixel values to the range [0,1]\n",
        "\tX = (X - X.min()) / (X.max() - X.min())\n",
        "\t# plot real images\n",
        "\tsquare = int(sqrt(n_samples))\n",
        "\tfor i in range(n_samples):\n",
        "\t\tpyplot.subplot(square, square, 1 + i)\n",
        "\t\tpyplot.axis('off')\n",
        "\t\tpyplot.imshow(X[i])\n",
        "\t# save plot to file\n",
        "\tfilename1 = 'plot_%s.png' % (name)\n",
        "\tpyplot.savefig(filename1)\n",
        "\tpyplot.close()\n",
        "\t# save the generator model\n",
        "\tfilename2 = 'model_%s.h5' % (name)\n",
        "\tg_model.save(filename2)\n",
        "\tprint('>Saved: %s and %s' % (filename1, filename2))\n",
        " "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nowTIqA-CW6q",
        "colab_type": "text"
      },
      "source": [
        "# training mode for different image sizes\n",
        "\n",
        "we start from 4x4 image size and train PGGAN model and then scale up images to 8x8, 16x16, 32x32, 64x64 and 128x128 sizes and extend train model for each\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odsMIwhfqcKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# train the generator and discriminator\n",
        "def train(g_models, d_models, gan_models, latent_dim, e_norm, e_fadein, n_batch):\n",
        "\t# fit the baseline model\n",
        "\tg_normal, d_normal, gan_normal = g_models[0][0], d_models[0][0], gan_models[0][0]\n",
        "\t# scale dataset to appropriate size\n",
        "\tgen_shape = g_normal.output_shape\n",
        "\t#scaled_data = scale_dataset(dataset, gen_shape[1:])\n",
        "\tprint('Scaled Data', gen_shape[1:])\n",
        "\t# train normal or straight-through models\n",
        "\ttrain_epochs(g_normal, d_normal, gan_normal, gen_shape[1:], e_norm[0], n_batch[0])\n",
        "\tsummarize_performance('tuned', g_normal, latent_dim)\n",
        "\t# process each level of growth\n",
        "\tfor i in range(1, len(g_models)):\n",
        "\t\t# retrieve models for this level of growth\n",
        "\t\t[g_normal, g_fadein] = g_models[i]\n",
        "\t\t[d_normal, d_fadein] = d_models[i]\n",
        "\t\t[gan_normal, gan_fadein] = gan_models[i]\n",
        "\t\t# scale dataset to appropriate size\n",
        "\t\tgen_shape = g_normal.output_shape\n",
        "\t\t#scaled_data = scale_dataset(dataset, gen_shape[1:])\n",
        "\t\tprint('Scaled Data', gen_shape[1:])\n",
        "\t\t# train fade-in models for next level of growth\n",
        "\t\ttrain_epochs(g_fadein, d_fadein, gan_fadein, gen_shape[1:], e_fadein[i], n_batch[i], True)\n",
        "\t\tsummarize_performance('faded', g_fadein, latent_dim)\n",
        "\t\t# train normal or straight-through models\n",
        "\t\ttrain_epochs(g_normal, d_normal, gan_normal, gen_shape[1:], e_norm[i], n_batch[i])\n",
        "\t\tsummarize_performance('tuned', g_normal, latent_dim)\n",
        " "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcAlmSlOqhnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of growth phases, e.g. 6 == [4, 8, 16, 32, 64, 128]\n",
        "n_blocks = 6\n",
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "# define models\n",
        "d_models = define_discriminator(n_blocks)\n",
        "# define models\n",
        "g_models = define_generator(latent_dim, n_blocks)\n",
        "# define composite models\n",
        "gan_models = define_composite(d_models, g_models)\n",
        "# load image data\n",
        "#dataset = load_real_samples('img_align_celeba_128.npz')\n",
        "#print('Loaded', dataset.shape)\n",
        "# train model\n",
        "n_batch = [16, 16, 16, 8, 4, 4]\n",
        "# 10 epochs == 500K images per training phase\n",
        "n_epochs = [5, 8, 8, 10, 10, 10]\n",
        "train(g_models, d_models, gan_models, latent_dim, n_epochs, n_epochs, n_batch)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}